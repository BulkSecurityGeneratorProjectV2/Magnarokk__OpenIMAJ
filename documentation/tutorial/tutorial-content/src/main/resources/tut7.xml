<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd">
<chapter id="sift-and-feature-matching">
  <title>SIFT and feature matching</title>
  <para>
    In this tutorial we’ll look at how to compare images to each other.
    Specifically, we’ll use a popular <emphasis role="strong">local
    feature descriptor</emphasis> called
    <emphasis role="strong">SIFT</emphasis> to extract some
    <emphasis>interesting points</emphasis> from images and describe
    them in a standard way. Once we have these local features and their
    descriptions, we can match local features to each other and
    therefore compare images to each other, or find a visual query image
    within a target image, as we will do in this tutorial.
  </para>
  <para>
    Firstly, lets load up a couple of images. Here we have a magazine
    and a scene containing the magazine:
  </para>
  <programlisting>
MBFImage query = ImageUtilities.readMBF(new URL(&quot;http://dl.dropbox.com/u/8705593/query.jpg&quot;));
	</programlisting>
  <para>
  </para>
  <programlisting>
MBFImage target = ImageUtilities.readMBF(new URL(&quot;http://dl.dropbox.com/u/8705593/target.jpg&quot;));
	</programlisting>
  <para>
    The first step is feature extraction. We’ll use the
    <emphasis role="strong">difference-of-Gaussian</emphasis> feature
    detector which we describe with a <emphasis role="strong">SIFT
    descriptor</emphasis>. The features we find are described in a way
    which makes them invariant to size changes, rotation and position.
    These are quite powerful features and are used in a variety of
    tasks. The standard implementation of SIFT in OpenIMAJ can be found
    in the <literal>DoGSIFTEngine</literal> class:
  </para>
  <programlisting>
DoGSIFTEngine engine = new DoGSIFTEngine();	
LocalFeatureList&lt;Keypoint&gt; queryKeypoints = engine.findFeatures(query.flatten());
LocalFeatureList&lt;Keypoint&gt; targetKeypoints = engine.findFeatures(target.flatten());
	</programlisting>
  <para>
    Once the engine is constructed, we can use it to extract
    <literal>Keypoint</literal> objects from our images. The
    <literal>Keypoint</literal> class contain a public field called
    <literal>ivec</literal> which, in the case of a standard SIFT
    descriptor is a 128 dimensional description of a patch of pixels
    around a detected point. Various distance measures can be used to
    compare <literal>Keypoint</literal>s to
    <literal>Keypoint</literal>s.
  </para>
  <para>
    The challenge in comparing <literal>Keypoint</literal>s is trying to
    figure out which <literal>Keypoint</literal>s match between
    <literal>Keypoint</literal>s from some query image and those from
    some target. The most basic approach is to take a given
    <literal>Keypoint</literal> in the query and find the
    <literal>Keypoint</literal> that is closest in the target. A minor
    improvement on top of this is to disregard those points which match
    well with MANY other points in the target. Such point are considered
    non-descriptive. Matching can be achieved in OpenIMAJ using the
    <literal>BasicMatcher</literal>. Next we’ll construct and setup such
    a matcher:
  </para>
  <programlisting>
LocalFeatureMatcher&lt;Keypoint&gt; matcher = new BasicMatcher&lt;Keypoint&gt;(80);
matcher.setModelFeatures(queryKeypoints);
matcher.findMatches(targetKeypoints);
	</programlisting>
  <para>
    We can now draw the matches between these two images found with this
    basic matcher using the <literal>MatchingUtilities</literal> class:
  </para>
  <programlisting>
MBFImage basicMatches = MatchingUtilities.drawMatches(query, target, matcher.getMatches(), RGBColour.RED);
DisplayUtilities.display(basicMatches);
	</programlisting>
  <para>
    As you can see, the basic matcher finds many matches, many of which
    are clearly incorrect. A more advanced approach is to filter the
    matches based on a given geometric model. One way of achieving this
    in OpenIMAJ is to use a
    <literal>ConsistentLocalFeatureMatcher</literal> which given an
    internal matcher, a geometric model and a model fitter, finds which
    matches given by the internal matcher are consistent with respect to
    the model and are therefore likely to be correct.
  </para>
  <para>
    To demonstrate this, we’ll use an algorithm called Random Sample
    Consensus (RANSAC) to fit a geometric model called an
    <emphasis role="strong">Affine transform</emphasis> to the initial
    set of matches. This is achieved by iteratively selecting a random
    set of matches, learning a model from this random set and then
    testing the remaining matches against the learnt model.
  </para>
  <para>
    We’ll now set up our model, our RANSAC model fitter and our
    consistent matcher:
  </para>
  <programlisting>
AffineTransformModel fittingModel = new AffineTransformModel(5);
RANSAC&lt;Point2d, Point2d&gt; ransac = new RANSAC&lt;Point2d, Point2d&gt;(fittingModel, 1500, new RANSAC.PercentageInliersStoppingCondition(0.5), true);

matcher = new ConsistentLocalFeatureMatcher2d&lt;Keypoint&gt;(
  new FastBasicKeypointMatcher&lt;Keypoint&gt;(8), ransac);

matcher.setModelFeatures(queryKeypoints);
matcher.findMatches(targetKeypoints);
MBFImage consistentMatches = MatchingUtilities.drawMatches(query, target, matcher.getMatches(), RGBColour.RED);
	</programlisting>
  <para>
  </para>
  <programlisting>
DisplayUtilities.display(consistentMatches);
	</programlisting>
  <para>
    The <literal>AffineTransformModel</literal> class models a
    two-dimensional Affine transform in OpenIMAJ. An interesting
    byproduct of this technique is that the
    <literal>AffineTransformModel</literal> contains the best transform
    matrix to go from the query to the target. We can take advantage of
    this by transforming the bounding box of our query with the
    transform estimated in the <literal>AffineTransformModel</literal>,
    therefore we can draw a polygon around the estimated location of the
    query within the target:
  </para>
  <programlisting>
target.drawShape(query.getBounds().transform(fittingModel.getTransform().inverse()), 3,RGBColour.BLUE);
DisplayUtilities.display(target); 
	</programlisting>
  <sect1 id="exercises-6">
    <title>Exercises</title>
    <sect2 id="exercise-1-different-matchers">
      <title>Exercise 1: Different matchers</title>
      <para>
        Experiment with different matchers; try the
        <literal>BasicTwoWayMatcher</literal> for example.
      </para>
    </sect2>
    <sect2 id="exercise-2-different-models">
      <title>Exercise 2: Different models</title>
      <para>
        Experiment with different models (such as a
        <literal>HomographyModel</literal>) in the consistent matcher.
      </para>
    </sect2>
  </sect1>
</chapter>
