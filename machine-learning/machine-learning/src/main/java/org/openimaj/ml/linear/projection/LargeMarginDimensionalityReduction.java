/**
 * Copyright (c) 2011, The University of Southampton and the individual contributors.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 *   * 	Redistributions of source code must retain the above copyright notice,
 * 	this list of conditions and the following disclaimer.
 *
 *   *	Redistributions in binary form must reproduce the above copyright notice,
 * 	this list of conditions and the following disclaimer in the documentation
 * 	and/or other materials provided with the distribution.
 *
 *   *	Neither the name of the University of Southampton nor the names of its
 * 	contributors may be used to endorse or promote products derived from this
 * 	software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
 * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
 * ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
package org.openimaj.ml.linear.projection;

import gnu.trove.list.array.TDoubleArrayList;

import org.openimaj.math.matrix.MatrixUtils;
import org.openimaj.math.matrix.algorithm.pca.ThinSvdPrincipalComponentAnalysis;

import Jama.Matrix;

/**
 * {@link LargeMarginDimensionalityReduction} is a technique to compress high
 * dimensional features into a lower-dimension representation using a learned
 * linear projection. Supervised learning is used to learn the projection such
 * that the squared Euclidean distance between two low-dimensional vectors is
 * less than a threshold if they correspond to the same object, or greater
 * otherwise. In addition, it is imposed that this condition is satisfied with a
 * margin of at least one.
 * <p>
 * In essence, the Euclidean distance in the low dimensional space produced by
 * this technique can be seen as a low-rank Mahalanobis metric in the original
 * space; the Mahalanobis matrix would have rank equal to the number of
 * dimensions of the smaller space.
 * <p>
 * This class implements the technique using stochastic sub-gradient descent. As
 * the objective function is not convex, initialisation is important, and
 * initial conditions are generated by selecting the largest PCA dimensions, and
 * then whitening the dimensions so they have equal magnitude. In addition, the
 * projection matrix is not regularised explicitly; instead the algorithm is
 * just stopped after a fixed number of iterations.
 * 
 * @author Jonathon Hare (jsh2@ecs.soton.ac.uk)
 */
public class LargeMarginDimensionalityReduction {
	public int ndims;
	public double wLearnRate = 1; // gamma
	public double bLearnRate = 0; // bias gamma
	public Matrix W;
	public double b;

	public LargeMarginDimensionalityReduction(int ndims) {
		this.ndims = ndims;
	}

	public void initialise(double[][] datai, double[][] dataj, boolean[] same) {
		final double[][] data = new double[2 * datai.length][];
		for (int i = 0; i < datai.length; i++) {
			data[2 * i] = datai[i];
			data[2 * i + 1] = dataj[i];
		}

		final ThinSvdPrincipalComponentAnalysis pca = new ThinSvdPrincipalComponentAnalysis(ndims);
		pca.learnBasis(data);
		W = pca.getBasis().times(MatrixUtils.diag(pca.getEigenValues()).inverse()).transpose();

		final TDoubleArrayList posDistances = new TDoubleArrayList();
		final TDoubleArrayList negDistances = new TDoubleArrayList();
		for (int i = 0; i < datai.length; i++) {
			if (same[i]) {
				posDistances.add(computeDistance(datai[i], dataj[i]));
			} else {
				negDistances.add(computeDistance(datai[i], dataj[i]));
			}
		}

		b = computeOptimal(posDistances, negDistances);
	}

	private double computeOptimal(TDoubleArrayList posDistances, TDoubleArrayList negDistances) {
		double bestAcc = 0;
		double bestThresh = -Double.MAX_VALUE;
		for (int i = 0; i < posDistances.size(); i++) {
			final double thresh = posDistances.get(i);

			final double acc = computeAccuracy(posDistances, negDistances, thresh);

			if (acc > bestAcc) {
				bestAcc = acc;
				bestThresh = thresh;
			}
		}

		for (int i = 0; i < negDistances.size(); i++) {
			final double thresh = negDistances.get(i);

			final double acc = computeAccuracy(posDistances, negDistances, thresh);

			if (acc > bestAcc) {
				bestAcc = acc;
				bestThresh = thresh;
			}
		}

		return bestThresh;
	}

	private double computeAccuracy(TDoubleArrayList posDistances, TDoubleArrayList negDistances, double thresh) {
		int correct = 0;
		for (int i = 0; i < posDistances.size(); i++) {
			if (posDistances.get(i) < thresh)
				correct++;
		}

		for (int i = 0; i < negDistances.size(); i++) {
			if (negDistances.get(i) >= thresh)
				correct++;
		}

		return (double) correct / (double) (posDistances.size() + negDistances.size());
	}

	private double computeDistance(double[] phii, double[] phij) {
		final Matrix diff = diff(phii, phij);

		return diff.transpose().times(W.transpose()).times(W).times(diff).get(0, 0);
	}

	private Matrix diff(double[] phii, double[] phij) {
		final Matrix diff = new Matrix(phii.length, 1);
		final double[][] diffv = diff.getArray();

		for (int i = 0; i < phii.length; i++) {
			diffv[i][0] = phii[i] - phij[i];
		}
		return diff;
	}

	private double sumsq(Matrix diffProj) {
		final double[][] v = diffProj.getArray();

		if (v[0].length != 1)
			throw new RuntimeException();

		double sumsq = 0;
		for (int i = 0; i < v.length; i++) {
			sumsq += v[i][0] * v[i][0];
		}

		return sumsq;
	}

	public boolean step(double[] phii, double[] phij, boolean same) {
		final int yij = same ? 1 : -1;

		final Matrix diff = diff(phii, phij);
		final Matrix diffProj = W.times(diff);
		final double sumsq = sumsq(diffProj);

		if (yij * (b - sumsq) > 1)
			return false;

		final Matrix updateW = diffProj.times(wLearnRate * yij).times(diff.transpose());

		W.minusEquals(updateW);

		b += yij * bLearnRate;

		return true;
	}

	public double[] project(double[] in) {
		return W.times(new Matrix(new double[][] { in }).transpose()).getColumnPackedCopy();
	}
}
