/**
 * Copyright (c) 2011, The University of Southampton and the individual contributors.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without modification,
 * are permitted provided that the following conditions are met:
 *
 *   * 	Redistributions of source code must retain the above copyright notice,
 * 	this list of conditions and the following disclaimer.
 *
 *   *	Redistributions in binary form must reproduce the above copyright notice,
 * 	this list of conditions and the following disclaimer in the documentation
 * 	and/or other materials provided with the distribution.
 *
 *   *	Neither the name of the University of Southampton nor the names of its
 * 	contributors may be used to endorse or promote products derived from this
 * 	software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR
 * ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
 * ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
package org.openimaj.ml.linear.projection;

import org.openimaj.math.matrix.MatrixUtils;
import org.openimaj.math.matrix.algorithm.pca.ThinSvdPrincipalComponentAnalysis;

import Jama.Matrix;

/**
 * {@link LargeMarginDimensionalityReduction} is a technique to compress high
 * dimensional features into a lower-dimension representation using a learned
 * linear projection. Supervised learning is used to learn the projection such
 * that the squared Euclidean distance between two low-dimensional vectors is
 * less than a threshold if they correspond to the same object, or greater
 * otherwise. In addition, it is imposed that this condition is satisfied with a
 * margin of at least one.
 * <p>
 * In essence, the Euclidean distance in the low dimensional space produced by
 * this technique can be seen as a low-rank Mahalanobis metric in the original
 * space; the Mahalanobis matrix would have rank equal to the number of
 * dimensions of the smaller space.
 * <p>
 * This class implements the technique using stochastic sub-gradient descent. As
 * the objective function is not convex, initialisation is important, and
 * initial conditions are generated by selecting the largest PCA dimensions, and
 * then whitening the dimensions so they have equal magnitude. In addition, the
 * projection matrix is not regularised explicitly; instead the algorithm is
 * just stopped after a fixed number of iterations.
 * 
 * @author Jonathon Hare (jsh2@ecs.soton.ac.uk)
 */
public class LargeMarginDimensionalityReduction {
	public int ndims;
	public double wLearnRate = 0.01;
	public double bLearnRate = 0;
	public Matrix W;
	public double b;

	public LargeMarginDimensionalityReduction(int ndims) {
		this.ndims = ndims;
	}

	public void initialise(double[][] data) {
		final ThinSvdPrincipalComponentAnalysis pca = new ThinSvdPrincipalComponentAnalysis(ndims);
		pca.learnBasis(data);

		W = pca.getBasis()
				.times(MatrixUtils.diag(pca.getEigenValues()).inverse()).transpose().times(100);

		double sum = 0;
		int count = 0;
		for (int i = 0; i < data.length; i++) {
			for (int j = i + 1; j < data.length; j++) {
				sum += computeDistance(data[i], data[j]);
				count++;
			}
		}

		b = 1000;// margin + sum / count;
	}

	private double computeDistance(double[] phii, double[] phij) {
		final Matrix diff = new Matrix(phii.length, 1);

		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		return diff.transpose().times(W.transpose()).times(W).times(diff).get(0, 0);
	}

	public boolean step(double[] phii, double[] phij, boolean same) {
		final int yij = same ? 1 : -1;

		if (yij * (b - computeDistance(phii, phij)) > 1)
			return false;

		System.out.println(same + " " + computeDistance(phii, phij));

		final Matrix diff = new Matrix(phii.length, 1);
		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		final Matrix psi = diff.times(diff.transpose());
		final Matrix updateW = W.times(psi).times(wLearnRate * yij);

		W.minusEquals(updateW);

		b -= yij * bLearnRate;

		System.out.println(MatrixUtils.toMatlabString(W));
		System.out.println(b);
		return true;
	}

	public double[] project(double[] in) {
		return W.times(new Matrix(new double[][] { in }).transpose()).getColumnPackedCopy();
	}
}
