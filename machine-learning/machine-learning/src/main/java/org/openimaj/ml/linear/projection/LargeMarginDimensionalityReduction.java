package org.openimaj.ml.linear.projection;

import org.openimaj.math.matrix.MatrixUtils;
import org.openimaj.math.matrix.algorithm.pca.ThinSvdPrincipalComponentAnalysis;

import Jama.Matrix;

/**
 * {@link LargeMarginDimensionalityReduction} is a technique to compress high
 * dimensional features into a lower-dimension representation using a learned
 * linear projection. Supervised learning is used to learn the projection such
 * that the squared Euclidean distance between two low-dimensional vectors is
 * less than a threshold if they correspond to the same object, or greater
 * otherwise. In addition, it is imposed that this condition is satisfied with a
 * margin of at least one.
 * <p>
 * In essence, the Euclidean distance in the low dimensional space produced by
 * this technique can be seen as a low-rank Mahalanobis metric in the original
 * space; the Mahalanobis matrix would have rank equal to the number of
 * dimensions of the smaller space.
 * <p>
 * This class implements the technique using stochastic sub-gradient descent. As
 * the objective function is not convex, initialisation is important, and
 * initial conditions are generated by selecting the largest PCA dimensions, and
 * then whitening the dimensions so they have equal magnitude. In addition, the
 * projection matrix is not regularised explicitly; instead the algorithm is
 * just stopped after a fixed number of iterations.
 * 
 * @author Jonathon Hare (jsh2@ecs.soton.ac.uk)
 */
public class LargeMarginDimensionalityReduction {
	public int ndims;
	public double wLearnRate = 0.01;
	public double bLearnRate = 0;
	public Matrix W;
	public double b;
	public Matrix WtW;

	public LargeMarginDimensionalityReduction(int ndims) {
		this.ndims = ndims;
	}

	public void initialise(double[][] data) {
		final ThinSvdPrincipalComponentAnalysis pca = new ThinSvdPrincipalComponentAnalysis(ndims);
		pca.learnBasis(data);

		W = pca.getBasis()
				.times(MatrixUtils.diag(pca.getEigenValues()).inverse()).transpose().times(100);
		WtW = W.transpose().times(W);

		double sum = 0;
		int count = 0;
		for (int i = 0; i < data.length; i++) {
			for (int j = i + 1; j < data.length; j++) {
				sum += computeDistance(data[i], data[j]);
				count++;
			}
		}

		b = 1000;// margin + sum / count;
	}

	private double computeDistance(double[] phii, double[] phij) {
		final Matrix diff = new Matrix(phii.length, 1);

		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		return diff.transpose().times(WtW).times(diff).get(0, 0);
	}

	public boolean step(double[] phii, double[] phij, boolean same) {
		final int yij = same ? 1 : -1;

		if (yij * (b - computeDistance(phii, phij)) > 1)
			return false;

		System.out.println(same + " " + computeDistance(phii, phij));

		final Matrix diff = new Matrix(phii.length, 1);
		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		final Matrix psi = diff.times(diff.transpose());
		final Matrix updateW = W.times(psi).times(wLearnRate * yij);

		W.minusEquals(updateW);
		WtW = W.transpose().times(W);

		b -= yij * bLearnRate;

		System.out.println(MatrixUtils.toMatlabString(W));
		System.out.println(b);
		return true;
	}

	public double[] project(double[] in) {
		return W.times(new Matrix(new double[][] { in }).transpose()).getColumnPackedCopy();
	}
}
