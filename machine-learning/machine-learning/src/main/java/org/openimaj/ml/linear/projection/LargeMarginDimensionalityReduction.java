package org.openimaj.ml.linear.projection;

import java.util.Arrays;
import java.util.Random;

import org.openimaj.image.DisplayUtilities;
import org.openimaj.image.MBFImage;
import org.openimaj.image.colour.ColourSpace;
import org.openimaj.image.colour.RGBColour;
import org.openimaj.math.geometry.shape.Circle;
import org.openimaj.math.matrix.MatrixUtils;
import org.openimaj.math.matrix.algorithm.pca.ThinSvdPrincipalComponentAnalysis;
import org.openimaj.math.statistics.distribution.SphericalMultivariateGaussian;

import Jama.Matrix;

/**
 * {@link LargeMarginDimensionalityReduction} is a technique to compress high
 * dimensional features into a lower-dimension representation using a learned
 * linear projection. Supervised learning is used to learn the projection such
 * that the squared Euclidean distance between two low-dimensional vectors is
 * less than a threshold if they correspond to the same object, or greater
 * otherwise. In addition, it is imposed that this condition is satisfied with a
 * margin of at least one.
 * <p>
 * In essence, the Euclidean distance in the low dimensional space produced by
 * this technique can be seen as a low-rank Mahalanobis metric in the original
 * space; the Mahalanobis matrix would have rank equal to the number of
 * dimensions of the smaller space.
 * <p>
 * This class implements the technique using stochastic sub-gradient descent. As
 * the objective function is not convex, initialisation is important, and
 * initial conditions are generated by selecting the largest PCA dimensions, and
 * then whitening the dimensions so they have equal magnitude. In addition, the
 * projection matrix is not regularised explicitly; instead the algorithm is
 * just stopped after a fixed number of iterations.
 * 
 * @author Jonathon Hare (jsh2@ecs.soton.ac.uk)
 */
public class LargeMarginDimensionalityReduction {
	int ndims;
	double wLearnRate = 0.01;
	double bLearnRate = 0;
	Matrix W;
	double b;
	Matrix WtW;
	private double margin = 1;

	LargeMarginDimensionalityReduction(int ndims) {
		this.ndims = ndims;
	}

	private void initialise(double[][] data) {
		final ThinSvdPrincipalComponentAnalysis pca = new ThinSvdPrincipalComponentAnalysis(ndims);
		pca.learnBasis(data);

		W = pca.getBasis()
				.times(MatrixUtils.diag(pca.getEigenValues()).inverse()).transpose().times(100);
		WtW = W.transpose().times(W);

		double sum = 0;
		int count = 0;
		for (int i = 0; i < data.length; i++) {
			for (int j = i + 1; j < data.length; j++) {
				sum += computeDistance(data[i], data[j]);
				count++;
			}
		}

		b = 1000;// margin + sum / count;
	}

	private double computeDistance(double[] phii, double[] phij) {
		final Matrix diff = new Matrix(phii.length, 1);

		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		return diff.transpose().times(WtW).times(diff).get(0, 0);
	}

	private boolean step(double[] phii, double[] phij, boolean same) {
		final int yij = same ? 1 : -1;

		if (yij * (b - computeDistance(phii, phij)) > margin)
			return false;

		System.out.println(same + " " + computeDistance(phii, phij));

		final Matrix diff = new Matrix(phii.length, 1);
		for (int i = 0; i < phii.length; i++) {
			diff.set(i, 0, phii[i] - phij[i]);
		}

		final Matrix psi = diff.times(diff.transpose());
		final Matrix updateW = W.times(psi).times(wLearnRate * yij);

		W.minusEquals(updateW);
		WtW = W.transpose().times(W);

		b -= yij * bLearnRate;

		System.out.println(MatrixUtils.toMatlabString(W));
		System.out.println(b);
		return true;
	}

	double[] project(double[] in) {
		return W.times(new Matrix(new double[][] { in }).transpose()).getColumnPackedCopy();
	}

	public static void main(String[] args) {
		final SphericalMultivariateGaussian g1 = new SphericalMultivariateGaussian(10);
		g1.variance = 8;
		g1.mean.getArray()[0][0] = 10;

		final SphericalMultivariateGaussian g2 = new SphericalMultivariateGaussian(10);
		g2.variance = 8;
		g2.mean.getArray()[0][1] = 10;

		final SphericalMultivariateGaussian g3 = new SphericalMultivariateGaussian(10);
		g3.variance = 8;
		g3.mean.getArray()[0][2] = 10;

		final Random rng = new Random();

		final double[][] data = new double[150][];
		for (int i = 0; i < data.length; i++) {
			if (i % 3 == 0) {
				data[i] = g1.sample(rng);
			} else if (i % 3 == 1) {
				data[i] = g2.sample(rng);
			} else {
				data[i] = g3.sample(rng);
			}
		}

		final LargeMarginDimensionalityReduction lmdr = new LargeMarginDimensionalityReduction(2);
		lmdr.initialise(data);

		System.out.println(MatrixUtils.toMatlabString(lmdr.W));
		System.out.println(lmdr.b);
		drawAllPoints(lmdr, data);

		int iter = 0;
		for (int o = 0; o < 10; o++) {
			for (int i = 0; i < data.length; i++) {
				for (int j = i + 1; j < data.length; j++) {
					if (lmdr.step(data[i], data[j], i % 3 == j % 3)) {
						System.out.println(iter);
						drawAllPoints(lmdr, data);
					}
					iter++;
				}
			}
		}
		System.out.println("done " + iter);
	}

	private static void drawAllPoints(LargeMarginDimensionalityReduction lmdr, double[][] data) {
		final MBFImage img = new MBFImage(500, 500, ColourSpace.RGB);

		for (int i = 0; i < data.length; i++) {
			final double[] pto = lmdr.project(data[i]);

			System.out.println(i % 2 + " " + Arrays.toString(pto));

			img.drawShapeFilled(new Circle((float) pto[0] + img.getWidth() / 2, (float) pto[1] + img.getHeight() / 2, 3),
					i % 3 == 0 ? RGBColour.RED : i % 3 == 1 ? RGBColour.GREEN : RGBColour.BLUE);
		}
		DisplayUtilities.displayName(img, "");
		try {
			Thread.sleep(200);
		} catch (final InterruptedException e) {
			// TODO Auto-generated catch block
			e.printStackTrace();
		}
	}
}
